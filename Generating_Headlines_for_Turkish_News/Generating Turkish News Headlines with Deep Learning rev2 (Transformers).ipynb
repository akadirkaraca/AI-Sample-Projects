{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KBMfBcP0NfI5",
        "GcPr0jGjP3kF"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow==2.4.1\n",
        "# !pip install keras==2.3.1"
      ],
      "metadata": {
        "id": "13GabJyc6VEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehcXYp_cCgOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "206ab26b-defa-4b6e-e672-74a3ba6505c8"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import torch, gc\n",
        "import datetime\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, CuDNNGRU\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#from tensorflow.python.keras.preprocessing.text import Tokenizer\"\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#from tensorflow.python.keras.preprocessing.sequence import pad_sequences\"\n",
        "from tensorflow.python.training import training\n",
        "from tensorflow.python.client import device_lib\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fr3q1sXUSOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9c46e5c-bca2-42ed-881a-1f3353fe5a11"
      },
      "source": [
        "try:\n",
        "  drive.mount('/content/drive')\n",
        "  os.chdir(\"drive/My Drive/GoogleColab/\")\n",
        "  print('Dosya dizini belirtilen konuma ayarlandı.')\n",
        "except:\n",
        "  print('Dosya dizini belirtilen konumda...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dosya dizini belirtilen konuma ayarlandı.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2NJq_XXGbla",
        "outputId": "dfefa4ba-5939-4536-a784-82c4f24efd18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jul  3 16:31:18 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z0T9ECuGeVW",
        "outputId": "59443b10-b190-4244-c416-a4942320b7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7Uth8v6cuc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6fd439-cb61-45be-c0fa-1fc13c947e11"
      },
      "source": [
        "try:\n",
        "  %load_ext tensorboard\n",
        "  print(\"TensorBoard eklentisi kuruldu.\")\n",
        "finally:\n",
        "  %rm -rf ./logs/\n",
        "  print(\"Önceki çalıştırmalardan elde edilen günlükler temizlendi.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorBoard eklentisi kuruldu.\n",
            "Önceki çalıştırmalardan elde edilen günlükler temizlendi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0C7QUAQCqVc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3280c6a3-104e-4ed1-88a1-94ed4d555f85"
      },
      "source": [
        "if tf.executing_eagerly():\n",
        "  print(\"İstekli çalışma modu çalışıyor.\")\n",
        "else:\n",
        "  tf.compat.v1.enable_eager_execution()\n",
        "  print(\"İstekli çalışma moduna geçildi.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "İstekli çalışma modu çalışıyor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGcKfdQdX5wB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d8aae62-1e2c-4598-9b1f-9d6e324ac031"
      },
      "source": [
        "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "print(\"Keras version: {}\".format(keras.__version__))\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.4.1\n",
            "Keras version: 2.3.1\n",
            "Eager execution: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I60tW0vL3vW"
      },
      "source": [
        "### **Verileri Yükleme**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU16wkuuZYgg"
      },
      "source": [
        "#document = pd.read_csv('train_document.csv')\n",
        "#print(\"train_document.csv dosyası içeri aktarıldı.\")\n",
        "#document.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "\n",
        "#news_texts = document['News']\n",
        "#title_texts = document['Title']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5HQAoW3fd_f"
      },
      "source": [
        "# document = pd.read_csv('Documents/documentRevV2.csv')\n",
        "# document.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "\n",
        "# news_texts = document['News']\n",
        "# title_texts = document['Title']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentPre = pd.read_csv('documentRevV2_3.csv')\n",
        "documentPre.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "document = documentPre[:125000]\n",
        "news_texts = document['News']\n",
        "title_texts = document['Title']\n",
        "\n",
        "series_news_texts = pd.Series(news_texts)\n",
        "series_title_texts = pd.Series(title_texts)"
      ],
      "metadata": {
        "id": "HrAh21z0VSKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbQqZR7fG-NZ"
      },
      "source": [
        "## **Eğitim Öncesi Ön İşlemler**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Haber ve Başlıklar için Metin Ön İşleme Aşaması\n",
        "\n",
        "Vektörizasyon, sekans uzunluklarının dolgulanması, vektörleri tensörlere dönüştürme ve dataset'in yığın halinde ayarlanması işlemleri..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Szninvjex52"
      },
      "source": [
        "with open(\"TurkishStopWords.txt\",mode=\"r\") as tsw:\n",
        "  stopWords = tsw.read()\n",
        "  stopWords = stopWords.split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUedXBDBU7bi"
      },
      "source": [
        "def news_preprocess(row):\n",
        "  # Sıra önemli\n",
        "  # Tarihleri silmek için (05.11.1994) (5 Kasım 1994)\n",
        "  row = re.sub(\"(([0-9]+(\\.)[0-9]+(\\.)[0-9]+)|([0-9]+.(ocak|şubat|mart|nisan|mayıs|haziran|temmuz|ağustos|eylül|ekim|kasım|aralık).[0-9]+)|([0-9]+.(ocak|şubat|mart|nisan|mayıs|haziran|temmuz|ağustos|eylül|ekim|kasım|aralık)))\\S+\", \"\", row, flags=re.IGNORECASE)\n",
        "  row=re.sub(r\"([0-9]+)\", ' ', str(row))\n",
        "  row = re.sub(\"([Hh][Aa][Bb][Ee][Rr][İi][Nn] [Dd][Ee][Vv][Aa][Mm][Iı])\\W+\",\" \",str(row)) # 'Haberin Devamı' yazan haberleri temizler \n",
        "  row=re.sub(\"([Kk]aynak: [abcçdefghıijklmnoöprsştuüvyzABCÇDEFGHIİJKLMNOÖPRSŞTUÜVYZ])\\w+\", ' ', str(row)) \n",
        "  row=re.sub(\"([İi]şte [Dd]etaylar)\\W+\", ' ', str(row))\n",
        "  row=re.sub(r\"(([ABCÇDEFGĞHIİJKLMNOÖPRSŞTUÜVYZ]{2,})+ ([ABCÇDEFGĞHIİJKLMNOÖPRSŞTUÜVYZ]{2,})+ ([ABCÇDEFGĞHIİJKLMNOÖPRSŞTUÜVYZ]{2,}))\\w+\", ' ', str(row)) # Ara başlıkları silmek için.\n",
        "  row=re.sub(r\"(([ABCÇDEFGĞHIİJKLMNOÖPRSŞTUÜVYZ]{2,})+ ([ABCÇDEFGĞHIİJKLMNOÖPRSŞTUÜVYZ]{2,}))\\w+\", ' ', str(row)) # Ara başlıkları silmek için.\n",
        "  row = re.sub(\"(İ)\",\"i\",str(row)) # Büyük İ sorunu için ASCII karşlığı yok\n",
        "  row = re.sub(\"(I)\",\"ı\",str(row)) # Büyük I sorunu için ASCII karşlığı yok\n",
        "  row=re.sub(\"(\\\\t)\", ' ', str(row)) # Tab'ları tek boşluk yapmak için\n",
        "  row=re.sub(\"(\\\\n)\", ' ', str(row))\n",
        "  row=re.sub(\"(\\\\r)\", ' ', str(row))\n",
        "  row = re.sub(\"(\\s+)\",' ',str(row)) # Birden fazla boşluğu tek boşluğa düşürmek için\n",
        "  row=re.sub(\"(__+)\", ' ', str(row))\n",
        "  row=re.sub(\"(--+)\", ' ', str(row))\n",
        "  row=re.sub(\"(~~+)\", ' ', str(row))\n",
        "  row=re.sub(\"(\\+\\++)\", ' ', str(row))\n",
        "  row=re.sub(\"(\\.\\.+)\", ' ', str(row))\n",
        "  row=re.sub(r\"[<>(){|}&©ø\\[\\]\\'\\\",;:?~*!#$%‘’+-.=@`]\", ' ', str(row)) # Karakter temizliği\n",
        "  row=re.sub(r\"[()]\\S+\", ' ', str(row)) # Parantez içini silmek için\n",
        "  # Çok sayıları tek sayı biçimine düşürmek için\n",
        "  row=re.sub(r\"([0-9]+)\", ' ', str(row))\n",
        "  # row=re.sub(\"(###)\\W+\", '### ', str(row))\n",
        "  row = re.sub(\"(\\s+)\",' ',str(row)) # Birden fazla boşluğu tek boşluğa düşürmek için\n",
        "  row=re.sub(r\"(\\s+.\\s+)\\w+\", ' ', str(row))\n",
        "  row = re.sub(\"(\\s+)\",' ',str(row))\n",
        "  row = row.strip() # Metinin sağ ve sol tarafındaki boşluğu silmek için\n",
        "  row = row.lower()\n",
        "\n",
        "  words_list = row.split()\n",
        "  filtered_stop_words = [word for word in words_list if word not in stopWords]\n",
        "  filtered_words = [word for word in filtered_stop_words if len(word)>2]\n",
        "\n",
        "  row = \" \".join(filtered_words)\n",
        "\n",
        "  return row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IerKqYPPVAA1"
      },
      "source": [
        "def title_preprocess(row):\n",
        "  # Sıra önemli\n",
        "  row = re.sub(\"(İ)\",\"i\",str(row)) # Büyük İ sorunu için ASCII karşlığı yok\n",
        "  row = re.sub(\"(I)\",\"ı\",str(row)) # Büyük I sorunu için ASCII karşlığı yok\n",
        "  row=re.sub(\"(\\\\t)\", ' ', str(row)) # Tab'ları tek boşluk yapmak için\n",
        "  row=re.sub(\"(\\\\n)\", ' ', str(row))\n",
        "  row=re.sub(\"(\\\\r)\", ' ', str(row))\n",
        "  row = re.sub(\"(\\s+)\",' ',str(row)) # Birden fazla boşluğu tek boşluğa düşürmek için\n",
        "  row=re.sub(\"(__+)\", ' ', str(row))\n",
        "  row=re.sub(\"(--+)\", ' ', str(row))\n",
        "  row=re.sub(\"(~~+)\", ' ', str(row))\n",
        "  row=re.sub(\"(\\+\\++)\", ' ', str(row))\n",
        "  row=re.sub(\"(\\.\\.+)\", ' ', str(row))\n",
        "  row=re.sub(r\"[<>(){|}&©ø\\[\\]\\'\\\",;:?~*!#$%‘’+-.=@`]\", ' ', str(row)) # Karakter temizliği\n",
        "  row=re.sub(r\"[()]\\S+\", ' ', str(row)) # Parantez içini silmek için\n",
        "  # Çok sayıları tek sayı biçimine düşürmek için\n",
        "  row=re.sub(r\"([0-9]+)\", '###', str(row))\n",
        "  row=re.sub(\"(###)\\W+\", '### ', str(row))\n",
        "  row = re.sub(\"(\\s+)\",' ',str(row)) # Birden fazla boşluğu tek boşluğa düşürmek için\n",
        "  row=re.sub(r\"(\\s+.\\s+)\\w+\", ' ', str(row))\n",
        "  row = re.sub(\"(\\s+)\",' ',str(row))\n",
        "  row = row.strip() # Metinin sağ ve sol tarafındaki boşluğu silmek için\n",
        "  row = row.lower()\n",
        " \n",
        "  return row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaNChp-rXOQM"
      },
      "source": [
        "# series_news_texts = pd.Series(news_texts)\n",
        "# series_title_texts = pd.Series(title_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1m83g7WH0f-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1031d9a5-f91c-4859-ffbf-28867b357bce"
      },
      "source": [
        "_ISTART_ = '<istart> '\n",
        "_IEND_ = ' <iend>'\n",
        "\n",
        "_TSTART_ = '<tstart> '\n",
        "_TEND_ = ' <tend>'\n",
        "\n",
        "tqdm.pandas(desc=\"Progress\")\n",
        "\n",
        "series_news_texts = series_news_texts.progress_apply(lambda x: _ISTART_ + news_preprocess(str(x)) + _IEND_).astype(str)\n",
        "series_title_texts = series_title_texts.progress_apply(lambda x: _TSTART_ + title_preprocess(str(x)) + _TEND_).astype(str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress: 100%|██████████| 120000/120000 [04:33<00:00, 438.00it/s]\n",
            "Progress: 100%|██████████| 120000/120000 [00:04<00:00, 27217.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preparedData = {'News':series_news_texts, 'Title':series_title_texts}\n",
        "# preparedSuDerDataset = pd.DataFrame(preparedData)\n",
        "# preparedSuDerDataset.to_csv(\"preparedSuDerDataset2.csv\")"
      ],
      "metadata": {
        "id": "p9vQaTehvwE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idM7UhL5urDb"
      },
      "source": [
        "**Dataset Train ve Validation Olarak Bölme**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Input ve Output'a göre vektörleri hazırlama\n",
        "encoder_maxlen = 384\n",
        "decoder_maxlen = 14\n",
        "\n",
        "short_text = []\n",
        "short_summary = []\n",
        "\n",
        "for i in range(len(series_news_texts)):\n",
        "    if(len(series_title_texts[i].split())<=decoder_maxlen and len(series_news_texts[i].split())<=encoder_maxlen):\n",
        "        short_text.append(series_news_texts[i])\n",
        "        short_summary.append(series_title_texts[i])\n",
        "\n",
        "short_text_series = pd.Series(short_text)\n",
        "short_summary_series = pd.Series(short_summary)\n",
        "\n",
        "news_train, news_validation, title_train, title_validation = train_test_split(short_text_series, short_summary_series, test_size=0.1, shuffle=False)"
      ],
      "metadata": {
        "id": "kbwWDl17WYlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(series_news_texts), len(short_text_series), len(series_title_texts), len(short_summary_series), "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPSPskzTD2PI",
        "outputId": "f3e37ed8-4626-4869-8689-c86354f93100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(120000, 111200, 120000, 111200)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# news_train, news_validation, title_train, title_validation = train_test_split(series_news_texts, series_title_texts, test_size=0.1)"
      ],
      "metadata": {
        "id": "iopOA3xluymU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMrgkfaYMzaw"
      },
      "source": [
        "**Metinleri tokenlere(jeton, vektör) Dönüştürme**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N35KRXEXtUOl"
      },
      "source": [
        "# '<' ve '>' varsayılan belirteçlerden kaldırılamaz. Çünkü dekoder için hedef derlemimizi bu işaretler arasına aldık\n",
        "# Gerekli olan filtreleme işlemini news_preprocess ve title_preprocess fonksionlarıyla yaptık\n",
        "filters = '!\"$%()*+,./:;=?@[\\\\]^_{|}~\\t\\n'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdXhzNaHsny9"
      },
      "source": [
        "#news_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters)\n",
        "#title_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters)\n",
        "news_tokenizer = keras.preprocessing.text.Tokenizer(filters=filters)\n",
        "title_tokenizer = keras.preprocessing.text.Tokenizer(filters=filters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzgHX8fCll37"
      },
      "source": [
        "print(\"Haber metinlerinin vektör sözlüğü oluşturuluyor...\")\n",
        "news_tokenizer.fit_on_texts(news_train)\n",
        "print(\"Haber metinlerinin vektör sözlüğü oluşturuldu.\")\n",
        "\n",
        "print(\"Haber başlıklarının vektör sözlüğü oluşturuluyor...\")\n",
        "# news_tokenizer.fit_on_texts(title_train)\n",
        "title_tokenizer.fit_on_texts(title_train)\n",
        "print(\"Haber başlıklarının vektör sözlüğü oluşturuldu.\")\n",
        "# --------------------------------------------------------------------\n",
        "# print(\"Haber metinlerinin vektör sözlüğü oluşturuluyor...\")\n",
        "# news_tokenizer.fit_on_texts(series_news_texts)\n",
        "# print(\"Haber metinlerinin vektör sözlüğü oluşturuldu.\")\n",
        "\n",
        "# print(\"Haber başlıklarının vektör sözlüğü oluşturuluyor...\")\n",
        "# news_tokenizer.fit_on_texts(series_title_texts)\n",
        "# print(\"Haber başlıklarının vektör sözlüğü oluşturuldu.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZf2sju4wdvO"
      },
      "source": [
        "print(\"Haber metinleri vektörleştiriliyor...\")\n",
        "inputs = news_tokenizer.texts_to_sequences(news_train)\n",
        "print(\"Haber metinleri vektörleştirildi.\")\n",
        "\n",
        "print(\"Haber başlıkları vektörleştiriliyor...\")\n",
        "# targets = news_tokenizer.texts_to_sequences(title_train)\n",
        "targets = title_tokenizer.texts_to_sequences(title_train)\n",
        "print(\"Haber başlıkları vektörleştirildi.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73NsAXe8UwpF"
      },
      "source": [
        "# news_tokenizer.texts_to_sequences([\"<istart>, <iend>\"]), title_tokenizer.texts_to_sequences([\"<tstart>, <tend>\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBG1ciE0mP4O",
        "outputId": "4fd0d24f-c449-4924-d94b-6c2cb76b4b03"
      },
      "source": [
        "encoder_vocab_size = len(news_tokenizer.word_index) + 1 \n",
        "decoder_vocab_size = len(title_tokenizer.word_index) + 1\n",
        "\n",
        "print('Encoder sözlük sayısı: {}\\nDecoder sözlük sayısı: {}'.format(encoder_vocab_size, decoder_vocab_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder sözlük sayısı: 410820\n",
            "Decoder sözlük sayısı: 56115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTObH43jR_iT"
      },
      "source": [
        "**Haber ve başlık metinlerinin ayrı ayrı uzunluklarını tanımlama**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWt_tqmjmWdN"
      },
      "source": [
        "news_train_lengths = pd.Series([len(x.split()) for x in news_train])\n",
        "title_train_lengths = pd.Series([len(x.split()) for x in title_train])\n",
        "\n",
        "news_validation_lengths = pd.Series([len(x.split()) for x in news_validation])\n",
        "title_validation_lengths = pd.Series([len(x.split()) for x in title_validation])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1S6wageSsyv"
      },
      "source": [
        "# print(\"Train Haber metni istatistikleri\")\n",
        "# news_train_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Validation Haber metni istatistikleri\")\n",
        "# news_validation_lengths.describe()"
      ],
      "metadata": {
        "id": "PZBS4sj8wJXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2XQ26H4cCK4"
      },
      "source": [
        "# print(\"Train Başlık metni istatistikleri\")\n",
        "# title_train_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Validation Başlık metni istatistikleri\")\n",
        "# title_validation_lengths.describe()"
      ],
      "metadata": {
        "id": "EFpPyZcbwPhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwriS3bRFYaU"
      },
      "source": [
        "ser = news_train_lengths\n",
        "tit = title_train_lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol_XEtnO23BH"
      },
      "source": [
        "# fig = plt.figure(dpi=1200)\n",
        "# ser.plot.kde(bw_method=0.3,grid=True).set_xlim(0,800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqUlku72FdRb"
      },
      "source": [
        "# ser.plot.kde(bw_method=0.3,grid=True).set_xlim(0,256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsYaVnEpF7y9"
      },
      "source": [
        "# fig = plt.figure(dpi=1200)\n",
        "# tit.plot.kde(bw_method=0.4,grid=True).set_xlim(0,16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAteUojAIJka"
      },
      "source": [
        "# plt.plot(news_train_lengths,)\n",
        "# plt.title(\"Haber Gövdesi Dağılım Grafiği\")\n",
        "# plt.xlabel(\"Haber Sayısı\")\n",
        "# plt.ylabel(\"Sözcük Sayısı\")\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSSytB5ZxbC2"
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    inputs,\n",
        "    maxlen=encoder_maxlen,\n",
        "    padding='post',\n",
        "    truncating='post')\n",
        "\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    targets,\n",
        "    maxlen=decoder_maxlen,\n",
        "    padding='post',\n",
        "    truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyEgdN4txbOa"
      },
      "source": [
        "# tf.cast almış olduğu x vektör dizisini (bu kod bloğunda inputs ve targets) tensör olarak döndürür. Tensörler numpy() dizileri gibi düşünülebilir\n",
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNQfaEClxbZB"
      },
      "source": [
        "BUFFER_SIZE = 5000\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yju1RmaBxk6B"
      },
      "source": [
        "**Eğitim için burada bi adımlama tekniği kullanacağımız için bölümleme içeriklerini de değiştireceğiz.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tslm5FLXxiEX",
        "outputId": "82167eff-1123-48b1-85f4-228e5e5e82c2"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 384]), TensorShape([64, 14]))"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBMfBcP0NfI5"
      },
      "source": [
        "# **Transformatör Mimarisi**\n",
        "---\n",
        "Pozisyonal kodlama, öz dikkat mekanizması, çok başlı dikkat mekanizması vs..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPWFGvAQx4Ox"
      },
      "source": [
        "**RNN'den farklı olarak kelimeler arasında konum kavramı eklemek için konumsal kodlama, bu yönsüzdür.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejs2FIGBOCX9"
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "  return position * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USpeS4S2O5uJ"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(\n",
        "      np.arange(position)[:, np.newaxis],\n",
        "      np.arange(d_model)[np.newaxis, :],\n",
        "      d_model)\n",
        "\n",
        "  # Dizideki bütün çift(2i) indeksli satırlara sinüs fonksiyonu uygulanır\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # Dizideki bütün tek(2i+1) indeksli satırlara kosinüs fonksiyonu uygulanır\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rFiq_NmyQUk"
      },
      "source": [
        "**Maskeleme**\n",
        "\n",
        "\n",
        "*   \"pad\" sekanslarını maskelemek için dolgu maskesi. (Sekansımızın hangi kısmı sonradan dolgu olarak eklenmiş)\n",
        "\n",
        "*  Mevcut sözcüklerin öz dikkat mekanizmasını kullanarak gelecek sözcük üretimine katkı sağlaması için ileri görüş maskelemesi sağlar.     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3Xdf3t6N7oF"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  return seq[:,tf.newaxis, tf.newaxis,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zlvBCf3QTyc"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJkSan2yy6kv"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1T7pTZsTlcA"
      },
      "source": [
        "### **Modeli Oluşturma**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWmCbZ0BUU7D"
      },
      "source": [
        "**Scaled Dot Product Attention (Ölçekli Nokta Çarpımı)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFf-GAHQUv6l"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)\n",
        "  return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSRfnnWoegux"
      },
      "source": [
        "**Multi-Head Attention (Çok Başlı Dikkat Mekanizması)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRaL6o44eu8a"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  #x bir tensördür.\n",
        "  def split_heads(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)\n",
        "    k = self.wk(k)\n",
        "    v = self.wv(v)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)\n",
        "    k = self.split_heads(k, batch_size)\n",
        "    v = self.split_heads(v, batch_size)\n",
        "    \n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "    output = self.dense(concat_attention)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JnwNAhQpYKT"
      },
      "source": [
        "**Feed Forward Network (İleri Besleme Ağı)**\n",
        "\n",
        "(Transformatör mimarisindeki katmanlarda yaptığımız işlemleri bir sonraki katmana iletmek için)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7ZIvNa0qifx"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([tf.keras.layers.Dense(dff, activation='relu'), tf.keras.layers.Dense(d_model)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5_TEESPUhn2"
      },
      "source": [
        "**Transformatör Mimarisi Encoder(Kodlayıcı) Temel Birimi**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlcB3XFvU7eL"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    attn_output, _ = self.mha(x, x, x, mask)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x+attn_output)\n",
        "\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1+ffn_output)\n",
        "\n",
        "    return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8yaJ8kDYs-e"
      },
      "source": [
        "**Transformatör Mimarisi Decoder(Kod Çözücü) Temel Birimi**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn83AePsYs-f"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "    ffn_output = self.ffn(out2)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77xrznpqoQ5T"
      },
      "source": [
        "**Çoklu Kodlayıcı Katmanlarından Oluşan Kodlayıcı (N x Encoder)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWDKhk3BoQ5U"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = Embedding(input_vocab_size, d_model) #trainable = True\n",
        "    \n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "  \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZn1yfMhoWpi"
      },
      "source": [
        "**Çoklu Kod Çözücü Katmanlarından Oluşan Kod Çözücü (N x Decoder)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hn6ZChkoWpi"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate = 0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = Embedding(target_vocab_size, d_model) # trainable=True\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model,tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "    return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcyV8qvhUsok"
      },
      "source": [
        "**Transformatör Sınıfı**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQO5SXchUsom"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "    dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "    final_output = self.final_layer(dec_output)\n",
        "\n",
        "    return final_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SESqjSBFPr0N"
      },
      "source": [
        "# **Eğitim**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCjJnkmSRokl"
      },
      "source": [
        "## **Kontrol Paneli**\n",
        "\n",
        "---\n",
        "(Hiperparametreler, optmizer, loss fonkiyonu ve transformatör ayarları)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ILm682qeNMT"
      },
      "source": [
        "# Hiperparametreler\n",
        "num_layers = 6\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "EPOCHS = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN4ASLf3eNMN"
      },
      "source": [
        "**Özel Öğrenme Oranıyla Adam Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlloyI2j7XYJ"
      },
      "source": [
        "# Serileştirilebilir Öğrenme Oranı Düşüş Programı.\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DphIGr5giSwO"
      },
      "source": [
        "**Kayıp Fonksiyonu Tanımlama ve Diğer Ölçümler**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbixIfCXiSwP"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ypuW0aFzby"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPj1bVOejp6p"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2MiJ687jsWH"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGFM2nr45RI9"
      },
      "source": [
        "def accuracy_function(real, pred):\n",
        "  pred = tf.cast(tf.argmax(pred[:][:], axis=-1), tf.int32)\n",
        "  accuracies = tf.equal(real, pred)\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1APN1NRWp7c"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fMEPGN3QElk"
      },
      "source": [
        "**Transfomatör**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqjMop7FrGYw"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads,\n",
        "    dff,\n",
        "    encoder_vocab_size,\n",
        "    decoder_vocab_size, \n",
        "    pe_input=encoder_vocab_size, \n",
        "    pe_target=decoder_vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Pn8lGZI1h0"
      },
      "source": [
        "## **Eğitim Adımı Fonksiyonu**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyhYvMhh_Un0"
      },
      "source": [
        "### **Kayıt ve Tensorboard Config**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XUDv5qTwyiz"
      },
      "source": [
        "checkpoint_path = \"checkpoints/rev2/transformers/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Son kayıt noktası düzenlendi!!')\n",
        "\n",
        "\n",
        "# checkpoint_path_epoch20 = \"checkpoints/test8/ckpt-10\"\n",
        "# ckpt.restore(checkpoint_path_epoch20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED8tmoNaYgJc"
      },
      "source": [
        "ckpt_manager.latest_checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca2lquF6Hydp",
        "outputId": "caa919b7-6b6c-4a7d-9c16-fa137d8c007b"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None, 384), (None, 14)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL1WsaHJb2n3"
      },
      "source": [
        "logdir = \"checkpoints/rev2/transformers/\"\n",
        "writer = tf.summary.create_file_writer(logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIUEueoLJZUo"
      },
      "source": [
        "### **Ana Eğitim Kod Bloğu**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIqhrWxSwzC5"
      },
      "source": [
        " @tf.function\n",
        " def train_step(inp, tar):\n",
        "   tar_inp = tar[:, :-1]\n",
        "   tar_real = tar[:, 1:]\n",
        "\n",
        "   enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "   with tf.GradientTape() as tape:\n",
        "\n",
        "     predictions, _ = transformer(inp,tar_inp,True,enc_padding_mask,combined_mask,dec_padding_mask)\n",
        "     loss = loss_function(tar_real, predictions)\n",
        "\n",
        "   gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "   optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "   train_loss(loss)\n",
        "   train_accuracy(accuracy_function(tar_real, predictions))\n",
        "   # train_accuracy(tar_real, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsaZqGM3si8X"
      },
      "source": [
        " train_total_time = []\n",
        "\n",
        " with writer.as_default():\n",
        "   for epoch in range(EPOCHS):\n",
        "\n",
        "     if epoch == 0:\n",
        "       print(\"Eğitim aşaması başladı...\")\n",
        "    \n",
        "     start = time.time()\n",
        "\n",
        "     train_loss.reset_states()\n",
        "     train_accuracy.reset_states()\n",
        "  \n",
        "     for (batch, (inp, tar)) in enumerate(dataset):\n",
        "       train_step(inp, tar)\n",
        "\n",
        "       if batch % 100 == 0: # if batch % 50 == 0:\n",
        "         print(\"Epoch: {} Batch: {} Doğruluk: {:.4f} Kayıp: {:.4f}\".format(epoch+1, batch, train_accuracy.result(), train_loss.result()))\n",
        "\n",
        "     tf.summary.scalar('Loss', data=float(train_loss.result()), step=epoch)\n",
        "     tf.summary.scalar('Accuracy', data=float(train_accuracy.result()), step=epoch)\n",
        "     writer.flush()\n",
        "    \n",
        "    #  if (epoch + 1) % 5 == 0:\n",
        "    #      ckpt_save_path = ckpt_manager.save()\n",
        "    #      print(\"{}. epoch'ta eğitim {} olarak kayıt edildi.\".format(epoch+1, ckpt_save_path))\n",
        "\n",
        "     ckpt_save_path = ckpt_manager.save()\n",
        "     print(\"{}. epoch'ta eğitim {} olarak kayıt edildi.\".format(epoch+1, ckpt_save_path))\n",
        "  \n",
        "     print(\"Epoch: {} Doğruluk: {:.4f} Kayıp: {:.4f}\".format(epoch+1, train_accuracy.result(), train_loss.result()))    \n",
        "  \n",
        "     epoch_duration = time.time() - start\n",
        "     train_total_time.append(epoch_duration)\n",
        "     print(\"Şuan ki epoch için geçen toplam süre: {:.4f} dk\".format(epoch_duration/60))\n",
        "     print(\"Toplam geçen süre: {:.4f} dk\".format(sum(train_total_time)/60))\n",
        "\n",
        "   if epoch == EPOCHS-1:\n",
        "     ckpt_save_path = ckpt_manager.save()\n",
        "     print(\"Eğitim sonu model {} olarak kayıt edildi.\".format(ckpt_save_path))\n",
        "     print(\"Eğitim aşaması bitti...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw0bzZusMb8P"
      },
      "source": [
        "# ckpt.save(\"model_train\")\n",
        "# os.listdir()\n",
        "# file.download(\"model_train-15.data-00000-of-00001\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxK_L-Ta2wKv"
      },
      "source": [
        "# **Sonuç**\n",
        "Kod çözücüde bir zamanda bir kelimeyi tahmin etmek ve onu çıktıya eklemek;\n",
        "\n",
        "daha sonra kod çözücüye dizinin(sekans) tamamını girdi olarak almak ve \\<stop> veya maxlen ile karşılaşana kadar aynı işleme devam etmek.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzs46jiPuG62"
      },
      "source": [
        "# Üretimsel Dekoder (Transformatör mimarisinin üretim tarafında kullandığı dekoder bloğu)\n",
        "def evaluate(input_document):\n",
        "  # inp_sentence = _ISTART_ + news_preprocess(input_document) + _IEND_\n",
        "  inp_sentence = input_document\n",
        "\n",
        "  input_document = news_tokenizer.texts_to_sequences([input_document])\n",
        "  input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "  encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "  decoder_input = [title_tokenizer.word_index[\"<tstart>\"]]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "  for i in range(decoder_maxlen):\n",
        "    enc_padding_mask, combined_mask, decoder_padding_mask = create_masks(encoder_input, output)\n",
        "        \n",
        "    predictions, attention_weights = transformer(\n",
        "        encoder_input,\n",
        "        output,\n",
        "        False,\n",
        "        enc_padding_mask,\n",
        "        combined_mask,\n",
        "        decoder_padding_mask)\n",
        "        \n",
        "    predictions = predictions[ : ,-1:, :]\n",
        "    \n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "        \n",
        "    if predicted_id == title_tokenizer.word_index[\"<tend>\"]:\n",
        "      return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "  return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS70UiWU4J6f"
      },
      "source": [
        "# Başlık üretmek için yazılan foksiyon input_document parametresi haber gövdesini temsil eder, return fonksiyonu ile haber gövdesi için üretilen başlık döndürülür.\n",
        "def generate_title(input_document):\n",
        "  generated_title = evaluate(input_document=input_document)[0].numpy()\n",
        "  generated_title = np.expand_dims(generated_title[1:],0) \n",
        "\n",
        "  return title_tokenizer.sequences_to_texts(generated_title)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_titles = []\n",
        "\n",
        "for i, news in enumerate(news_validation):\n",
        "  generated_title = generate_title(news)\n",
        "  generated_titles.append(generated_title)\n",
        "\n",
        "generated_titles_series = pd.Series(generated_titles)"
      ],
      "metadata": {
        "id": "BRirWqabpxLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_validation_generated = {\"News\":news_validation, \"OriginalTitle\":title_validation, \"Predictions\":generated_titles}\n",
        "df_validation_generated = pd.DataFrame(dict_validation_generated)\n",
        "df_validation_generated.to_csv(\"GeneratedTitlesTransformers.csv\")"
      ],
      "metadata": {
        "id": "UZ9e4s26rfVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcPr0jGjP3kF"
      },
      "source": [
        "## **Computing Score**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmeVu9E93y11"
      },
      "source": [
        "### **BLEU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmulKgj6etYA"
      },
      "source": [
        "!pip install --upgrade bleu\n",
        "from bleu import list_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uRuPmwR32f7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U61vrCFUlQtW"
      },
      "source": [
        "ref = [\"\"\"thy nin yolcu sayısı ### milyona ulaştı\"\"\"]\n",
        "hyp = [\"\"\"THY yolcu sayısını 9.2 milyona çıkardı\"\"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRcL8nizOHC6"
      },
      "source": [
        "list_bleu([ref], hyp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PzjJaYk4CWl"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TKNYPySlv_X"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VA0VvqMlGOk"
      },
      "source": [
        "hypothesis = \"### terörist etkisiz hale getirildi\".split()\n",
        "reference = title_preprocess(\"Bitlis'te 6 teröristin öldürüldüğü operasyonda çok sayıda silah mühimmat ele geçirildi\").split()\n",
        "#there may be several references\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,auto_reweigh=True)\n",
        "print(BLEUscore*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaI_pZ_dQas1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u825Q0MAQ3nW"
      },
      "source": [
        "# two references for one document\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "references = [[\"\".split()]]\n",
        "candidates = [\"\".split(), \"\".split()]\n",
        "score = corpus_bleu(references, candidates)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au7New1G38qR"
      },
      "source": [
        "### **ROUGE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-yiF5xP9sq7"
      },
      "source": [
        "!pip install 0 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzcvzawjjEJH"
      },
      "source": [
        "from rouge_metric import PyRouge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZQ4057r97Ox"
      },
      "source": [
        "By default, sentences are separated by '\\n' and tokens are separated by white space in a document. This tokenization process can be further customized. For example,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwrwbETi99fn"
      },
      "source": [
        "# # Template\n",
        "# from rouge_metric import PyRouge\n",
        "\n",
        "# # Pre-process and tokenize the summaries as you like\n",
        "# hypotheses = [\n",
        "#     ['how are you'.split(), 'i am fine'.split()],                       # document 1: hypothesis\n",
        "#     ['it is fine today'.split(), 'we won the football game'.split()],   # document 2: hypothesis\n",
        "# ]\n",
        "# references = [[\n",
        "#     ['how do you do'.split(), 'fine thanks'.split()],   # document 1: reference 1\n",
        "#     ['how old are you'.split(), 'i am three'.split()],  # document 1: reference 2\n",
        "# ], [\n",
        "#     ['it is sunny today'.split(), 'let us go for a walk'.split()],  # document 2: reference 1\n",
        "#     ['it is a terrible day'.split(), 'we lost the game'.split()],   # document 2: reference 2\n",
        "# ]]\n",
        "\n",
        "# # Evaluate on tokenized documents\n",
        "# rouge = PyRouge(rouge_n=(1, 2, 4), rouge_l=True, rouge_w=True,\n",
        "#                 rouge_w_weight=1.2, rouge_s=True, rouge_su=True, skip_gap=4)\n",
        "# scores = rouge.evaluate_tokenized(hypotheses, references)\n",
        "# print(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgkWFSYBjrmm"
      },
      "source": [
        "# hypotheses = [\"kavga ### can alan maganda vurdu\".split()]\n",
        "# references = [\"Kavga sebebi sudan faturası ise iki can\".split()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz8QqATdkQ5s"
      },
      "source": [
        "# print(hypotheses)\n",
        "# print(references)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBMUse4rkXLk"
      },
      "source": [
        "# references = [title.split() for title in title_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNJaKiFYknfT"
      },
      "source": [
        "# hypotheses20 = [generate_title(news_preprocess(news)).split() for news in news_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O6m4e0_FIvI7"
      },
      "source": [
        "# hypotheses20 = news_texts.progress_apply(lambda x: generate_title(news_preprocess(str(x))) if i < range(25000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJwL6Kkc1Kgd"
      },
      "source": [
        "7poıuytğü\n",
        "hypotheses25 = []\n",
        "references = []\n",
        "\n",
        "# tqdm.pandas(desc=\"Progress\")\n",
        "# for i in tqdm(range(25000,52489)):\n",
        "#   hypotheses20.append(generate_title(news_preprocess(news_texts[i])).split())\n",
        "#   references.append(title_texts[i].split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSQtV6eY4Vnc"
      },
      "source": [
        "dict_rouge = {\"Tahmin - 25 Epoch\":hypotheses25}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIKZjKttVQZ0"
      },
      "source": [
        "df = pd.DataFrame(dict_rouge)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIljRtUVbpC"
      },
      "source": [
        "df.to_csv(\"s25k25e.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chsb1eB5VgkC",
        "outputId": "e0da4dd7-75ca-48d5-fc5f-1f269fdaa4a0"
      },
      "source": [
        "for i in tqdm(range(25000,52489)):\n",
        "  hypotheses25.append(generate_title(news_preprocess(news_texts[i])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 27489/27489 [9:17:10<00:00,  1.22s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}